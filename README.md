# ğŸ“¦ Job Scraper Pipeline

**A modular, scalable ETL system for job scraping across Middle East job boards.**
Designed for Data Engineering & Python practice using real-world scraping and pipeline architecture.

---

<p align="center">

  <!-- Build badges -->

  <img src="https://img.shields.io/badge/Python-3.10%2B-blue" />
  <img src="https://img.shields.io/badge/Scraping-Polite%20%26%20Modular-green" />
  <img src="https://img.shields.io/badge/Database-SQLite-lightgrey" />
  <img src="https://img.shields.io/badge/ETL-Pipeline-orange" />
  <img src="https://img.shields.io/badge/Status-Under%20Active%20Development-yellow" />

</p>

---

# ğŸš€ Overview

This project is a **production-ready job scraping pipeline** organized into a clean ETL architecture:

**Extract â†’ Transform â†’ Load â†’ Automate**

You can scrape multiple job boards (Wuzzuf, GulfTalent, Bayt, etc.), clean and normalize the data, extract metadata (tags, salary, seniority), dedupe jobs, load into SQLite/CSV, and schedule daily runs.

The entire pipeline is fully modular, configurable, and extendable â€” every site scraper lives in its own file.

---

# ğŸ“‚ Project Structure

```
job-scraper/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â”œâ”€â”€ cli.py
â”‚
â”œâ”€â”€ extract/
â”‚   â”œâ”€â”€ base_scraper.py
â”‚   â”œâ”€â”€ wuzzuf.py
â”‚   â”œâ”€â”€ gulftalent.py
â”‚   â”œâ”€â”€ naukrigulf.py
â”‚   â”œâ”€â”€ tanqeeb.py
â”‚   â”œâ”€â”€ drjobs.py
â”‚   â”œâ”€â”€ bayt.py
â”‚   â”œâ”€â”€ laimoon.py
â”‚   â”œâ”€â”€ akhtaboot.py
â”‚   â”œâ”€â”€ example_site.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ fetch.py
â”‚       â”œâ”€â”€ rate_limit.py
â”‚       â”œâ”€â”€ parse.py
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ transform/
â”‚   â”œâ”€â”€ normalize.py
â”‚   â”œâ”€â”€ clean_text.py
â”‚   â”œâ”€â”€ extract_metadata.py
â”‚   â”œâ”€â”€ dedupe.py
â”‚   â””â”€â”€ text_normalization/
â”‚       â”œâ”€â”€ arabic.py
â”‚       â”œâ”€â”€ english.py
â”‚       â”œâ”€â”€ html.py
â”‚       â””â”€â”€ unicode.py
â”‚
â”œâ”€â”€ load/
â”‚   â”œâ”€â”€ to_csv.py
â”‚   â”œâ”€â”€ to_sqlite.py
â”‚   â”œâ”€â”€ to_parquet.py
â”‚   â”œâ”€â”€ merge.py
â”‚   â””â”€â”€ schema.py
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ runner.py
â”‚   â”œâ”€â”€ scheduler.py
â”‚   â””â”€â”€ validation.py
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ helpers.py
â”‚   â””â”€â”€ exceptions.py
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ sites.yml
â”‚   â””â”€â”€ rules/
â”‚       â”œâ”€â”€ salary.yml
â”‚       â”œâ”€â”€ seniority.yml
â”‚       â””â”€â”€ tags.yml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ run_history.json
â”‚   â”œâ”€â”€ cache/
â”‚   â””â”€â”€ mapping/
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ run_pipeline.sh
```

---

# ğŸ›  Installation

## 1ï¸âƒ£ Clone the repo

```bash
git clone https://github.com/husseini2000/job-scraper.git
cd job-scraper
```

## 2ï¸âƒ£ Create a virtual environment

```bash
python -m venv env
source env/bin/activate        # Mac/Linux
env\Scripts\activate           # Windows
```

## 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

Or if you're using Poetry:

```bash
poetry install
```

---

# â–¶ï¸ Quick Start

### **Run entire ETL pipeline**

```bash
python cli.py run-all
```

### **Run per-stage**

```bash
python cli.py extract
python cli.py transform
python cli.py load
```

### **Scrape one site**

```bash
python cli.py extract --site wuzzuf
```

---

# ğŸ§ª Testing

```bash
pytest -q
```

---

# ğŸ¯ Features

### âœ… Modular scrapers

Each job site has its own Python scraper.

### âœ… Config-driven

Enable/disable sites or adjust rate limits in:

```
configs/sites.yml
```

### âœ… Strong Transform Layer

* HTML & emoji cleaning
* Arabic + English normalization
* Salary extraction
* Seniority detection
* Skill tag extraction (Python, SQL, AWS, Airflow, etc.)
* Duplicate job removal

### âœ… Multiple Load Targets

* CSV
* SQLite
* Parquet

### âœ… Pipeline Automation

Use `pipeline/runner.py` or run via cron using:

```
scripts/run_pipeline.sh
```

---

# ğŸ§­ Roadmap

This project is divided into phases to help build a strong, production-worthy pipeline.

---

## ğŸ§± **PHASE 0 â€” Foundation & Environment (1 day)**

**Goal:** Prepare the project structure and development environment.

### Tasks

* Initialise Git repo
* Create the directory structure
* Add `.gitignore`, `requirements.txt`, `pyproject.toml` (optional)
* Setup virtualenv
* Add preliminary `README.md`
* Create `Makefile`

**Output:** Skeleton project.

---

## ğŸ£ **PHASE 1 â€” Core Engine & Utilities (2â€“3 days)**

**Goal:** Create the shared engine for all scrapers.

### Tasks

* Implement utils (fetcher, rate limiter, logger)
* Implement core models and helpers
* Create `configs/sites.yml`

**Output:** Full scraper engine foundation.

---

## ğŸŒ **PHASE 2 â€” First Scraper (Wuzzuf) + BaseScraper (3â€“4 days)**

Build BaseScraper and implement Wuzzuf as the first complete scraper.

**Output:** Working Wuzzuf scraper.

---

## ğŸ§¹ **PHASE 3 â€” Transform Layer (4â€“5 days)**

Clean, normalize, extract metadata, dedupe.

**Output:** Standardized job objects.

---

## ğŸ›¢ **PHASE 4 â€” Load Layer (2â€“3 days)**

CSV, SQLite, Parquet, merging, schema.

**Output:** jobs_raw.csv, jobs_clean.csv, jobs.db

---

## ğŸŒ **PHASE 5 â€” Additional Scrapers (5â€“12 days)**

GulfTalent, Tanqeeb, DrJobs, Bayt, NaukriGulf, Laimoon, Akhtaboot.

---

## ğŸ” **PHASE 6 â€” CLI + Pipeline Runner (2â€“3 days)**

One-command ETL workflow.

---

## ğŸ“Š **PHASE 7 â€” Validation, Logging, Monitoring (1â€“2 days)**

Add run history, validation, and clear error reporting.

---

## ğŸš€ **PHASE 8 â€” Automation & Deployment (1 day)**

Cron job automation + final polish.

---

# â¤ï¸ Contributing

Pull requests are welcome.
If you're scrapers for additional job sites, follow the `example_site.py` template.

---

# ğŸ“œ License

MIT License.

---
